{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa bibliotecas utilizadas\n",
    "import numpy as np\n",
    "import time as tm\n",
    "import math\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "\n",
    "###start here\n",
    "\"\"\"\n",
    "Parameters\n",
    "K - size of x\n",
    "N - size of y\n",
    "snrdb_low - the lower bound of noise db used during training\n",
    "snr_high - the higher bound of noise db used during training\n",
    "L - number of layers in DetNet\n",
    "v_size = size of auxiliary variable at each layer\n",
    "hl_size - size of hidden layer at each DetNet layer (the dimention the layers input are increased to\n",
    "startingLearningRate - the initial step size of the gradient descent algorithm\n",
    "decay_factor & decay_step_size - each decay_step_size steps the learning rate decay by decay_factor\n",
    "train_iter - number of train iterations\n",
    "train_batch_size - batch size during training phase\n",
    "test_iter - number of test iterations\n",
    "test_batch_size  - batch size during testing phase\n",
    "LOG_LOSS - equal 1 if loss of each layer should be sumed in proportion to the layer depth, otherwise all losses have the same weight \n",
    "res_alpha- the proportion of the previuos layer output to be added to the current layers output (view ResNet article)\n",
    "snrdb_low_test & snrdb_high_test & num_snr - when testing, num_snr different SNR values will be tested, uniformly spread between snrdb_low_test and snrdb_high_test \n",
    "By Neev Samuel neev(dot)samuel(at)gmail(dot)com\n",
    "\"\"\"\n",
    "#Instância sessão no tf\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "#Parâmetros\n",
    "K = 20\n",
    "N = 30\n",
    "snrdb_low = 7.0\n",
    "snrdb_high = 14.0\n",
    "snr_low = 10.0 ** (snrdb_low/10.0)\n",
    "snr_high = 10.0 ** (snrdb_high/10.0)\n",
    "L=90\n",
    "v_size = 2*K\n",
    "hl_size = 8*K\n",
    "startingLearningRate = 0.0001\n",
    "decay_factor = 0.97\n",
    "decay_step_size = 1000\n",
    "train_iter = 20000\n",
    "train_batch_size = 5000\n",
    "test_iter= 200\n",
    "test_batch_size = 1000\n",
    "LOG_LOSS = 1\n",
    "res_alpha=0.9\n",
    "num_snr = 6\n",
    "snrdb_low_test=8.0\n",
    "snrdb_high_test=13.0\n",
    "\n",
    "\"\"\"Data generation for train and test phases\n",
    "In this example, both functions are the same.\n",
    "This duplication is in order to easily allow testing cases where the test is over different distributions of data than in the training phase.\n",
    "e.g. training over gaussian i.i.d. channels and testing over a specific constant channel.\n",
    "currently both test and train are over i.i.d gaussian channel.\n",
    "\"\"\"\n",
    "\n",
    "#Geração de dados para teste\n",
    "def generate_data_iid_test(B,K,N,snr_low,snr_high):\n",
    "    H_=np.random.randn(B,N,K)\n",
    "    W_=np.zeros([B,K,K])\n",
    "    x_=np.sign(np.random.rand(B,K)-0.5)\n",
    "    y_=np.zeros([B,N])\n",
    "    w=np.random.randn(B,N)\n",
    "    Hy_=x_*0\n",
    "    HH_=np.zeros([B,K,K])\n",
    "    SNR_= np.zeros([B])\n",
    "    for i in range(B):\n",
    "        SNR = np.random.uniform(low=snr_low,high=snr_high)\n",
    "        H=H_[i,:,:]\n",
    "        tmp_snr=(H.T.dot(H)).trace()/K\n",
    "        H_[i,:,:]=H\n",
    "        y_[i,:]=(H.dot(x_[i,:])+w[i,:]*np.sqrt(tmp_snr)/np.sqrt(SNR))\n",
    "        Hy_[i,:]=H.T.dot(y_[i,:])\n",
    "        HH_[i,:,:]=H.T.dot( H_[i,:,:])\n",
    "        SNR_[i] = SNR\n",
    "    return y_,H_,Hy_,HH_,x_,SNR_\n",
    "\n",
    "#Gerar dados de treinamento\n",
    "def generate_data_train(B,K,N,snr_low,snr_high):\n",
    "    H_=np.random.randn(B,N,K)\n",
    "    W_=np.zeros([B,K,K])\n",
    "    x_=np.sign(np.random.rand(B,K)-0.5)\n",
    "    y_=np.zeros([B,N])\n",
    "    w=np.random.randn(B,N)\n",
    "    Hy_=x_*0\n",
    "    HH_=np.zeros([B,K,K])\n",
    "    SNR_= np.zeros([B])\n",
    "    for i in range(B):\n",
    "        SNR = np.random.uniform(low=snr_low,high=snr_high)\n",
    "        H=H_[i,:,:]\n",
    "        tmp_snr=(H.T.dot(H)).trace()/K\n",
    "        H_[i,:,:]=H\n",
    "        y_[i,:]=(H.dot(x_[i,:])+w[i,:]*np.sqrt(tmp_snr)/np.sqrt(SNR))\n",
    "        Hy_[i,:]=H.T.dot(y_[i,:])\n",
    "        HH_[i,:,:]=H.T.dot( H_[i,:,:])\n",
    "        SNR_[i] = SNR\n",
    "    return y_,H_,Hy_,HH_,x_,SNR_\n",
    " \n",
    "def piecewise_linear_soft_sign(x):\n",
    "    t = tf.Variable(0.1)\n",
    "    y = -1+tf.nn.relu(x+t)/(tf.abs(t)+0.00001)-tf.nn.relu(x-t)/(tf.abs(t)+0.00001)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def affine_layer(x,input_size,output_size,Layer_num):\n",
    "    W = tf.Variable(tf.random_normal([input_size, output_size], stddev=0.01))\n",
    "    w = tf.Variable(tf.random_normal([1, output_size], stddev=0.01))\n",
    "    y = tf.matmul(x, W)+w\n",
    "    return y\n",
    "\n",
    "def relu_layer(x,input_size,output_size,Layer_num):\n",
    "    y = tf.nn.relu(affine_layer(x,input_size,output_size,Layer_num))\n",
    "    return y\n",
    "\n",
    "def sign_layer(x,input_size,output_size,Layer_num):\n",
    "    y = piecewise_linear_soft_sign(affine_layer(x,input_size,output_size,Layer_num))\n",
    "    return y\n",
    "\n",
    "HY = tf.placeholder(tf.float32,shape=[None,K])\n",
    "X = tf.placeholder(tf.float32,shape=[None,K])\n",
    "HH = tf.placeholder(tf.float32,shape=[None, K , K])\n",
    "\n",
    "batch_size = tf.shape(HY)[0]\n",
    "\n",
    "X_LS = tf.matmul(tf.expand_dims(HY,1),tf.matrix_inverse(HH))\n",
    "X_LS= tf.squeeze(X_LS,1)\n",
    "loss_LS = tf.reduce_mean(tf.square(X - X_LS))\n",
    "ber_LS = tf.reduce_mean(tf.cast(tf.not_equal(X,tf.sign(X_LS)), tf.float32))\n",
    "\n",
    "S=[]\n",
    "S.append(tf.zeros([batch_size,K]))\n",
    "V=[]\n",
    "V.append(tf.zeros([batch_size,v_size]))\n",
    "LOSS=[]\n",
    "LOSS.append(tf.zeros([]))\n",
    "BER=[]\n",
    "BER.append(tf.zeros([]))\n",
    "\n",
    "\n",
    "#The architecture of DetNet\n",
    "for i in range(1,L):\n",
    "    temp1 = tf.matmul(tf.expand_dims(S[-1],1),HH)\n",
    "    temp1= tf.squeeze(temp1,1)\n",
    "    Z = tf.concat([HY,S[-1],temp1,V[-1]],1)\n",
    "    ZZ = relu_layer(Z,3*K + v_size , hl_size,'relu'+str(i))\n",
    "    S.append(sign_layer(ZZ , hl_size , K,'sign'+str(i)))\n",
    "    S[i]=(1-res_alpha)*S[i]+res_alpha*S[i-1]\n",
    "    V.append(affine_layer(ZZ , hl_size , v_size,'aff'+str(i)))\n",
    "    V[i]=(1-res_alpha)*V[i]+res_alpha*V[i-1]  \n",
    "    if LOG_LOSS == 1:\n",
    "        LOSS.append(np.log(i)*tf.reduce_mean(tf.reduce_mean(tf.square(X - S[-1]),1)/tf.reduce_mean(tf.square(X - X_LS),1)))\n",
    "    else:\n",
    "        LOSS.append(tf.reduce_mean(tf.reduce_mean(tf.square(X - S[-1]),1)/tf.reduce_mean(tf.square(X - X_LS),1)))          \n",
    "    BER.append(tf.reduce_mean(tf.cast(tf.not_equal(X,tf.sign(S[-1])), tf.float32)))\n",
    "\n",
    "TOTAL_LOSS=tf.add_n(LOSS)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(startingLearningRate, global_step, decay_step_size, decay_factor, staircase=True)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(TOTAL_LOSS)\n",
    "init_op=tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init_op)\n",
    "\n",
    "#Training DetNet\n",
    "for i in range(train_iter): \n",
    "    batch_Y, batch_H, batch_HY, batch_HH, batch_X , SNR1= generate_data_train(train_batch_size,K,N,snr_low,snr_high)\n",
    "    \n",
    "    train_step.run(feed_dict={HY: batch_HY, HH: batch_HH, X: batch_X})\n",
    "    \n",
    "    if i % 100 == 0 : \n",
    "        batch_Y, batch_H, batch_HY, batch_HH, batch_X ,SNR1= generate_data_iid_test(train_batch_size,K,N,snr_low,snr_high)\n",
    "        results = sess.run([loss_LS,LOSS[L-1],ber_LS,BER[L-1]], {HY: batch_HY, HH: batch_HH, X: batch_X})\n",
    "        print_string = [i]+results\n",
    "        print ((' ').join('%s' % x for x in print_string))\n",
    "          \n",
    "snrdb_list = np.linspace(snrdb_low_test,snrdb_high_test,num_snr)\n",
    "snr_list = 10.0 ** (snrdb_list/10.0)\n",
    "\n",
    "bers = np.zeros((1,num_snr))\n",
    "times = np.zeros((1,num_snr))\n",
    "tmp_bers = np.zeros((1,test_iter))\n",
    "tmp_times = np.zeros((1,test_iter))\n",
    "\n",
    "for j in range(num_snr):\n",
    "    for jj in range(test_iter):\n",
    "        print('snr:')\n",
    "        print(snrdb_list[j])\n",
    "        print('test iteration:')\n",
    "        print(jj)\n",
    "        \n",
    "        batch_Y, batch_H, batch_HY, batch_HH, batch_X ,SNR1= generate_data_iid_test(test_batch_size , K,N,snr_list[j],snr_list[j])\n",
    "        tic = tm.time()\n",
    "        \n",
    "        tmp_bers[:,jj] = np.array(sess.run(BER[L-1], {HY: batch_HY, HH: batch_HH, X: batch_X}))    \n",
    "        toc = tm.time()\n",
    "        tmp_times[0][jj] =toc - tic\n",
    "        \n",
    "        \n",
    "    bers[0][j] = np.mean(tmp_bers,1)\n",
    "    times[0][j] = np.mean(tmp_times[0])/test_batch_size\n",
    "\n",
    "print('snrdb_list')\n",
    "print(snrdb_list)\n",
    "print('bers')\n",
    "print(bers)\n",
    "print('times')\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
